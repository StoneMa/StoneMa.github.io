---
title: MachineLearning之SVM
date: 2018-04-17 17:23:54
tags: [机器学习,SVM,有监督学习,Supervised Learning]
categories: 机器学习
mathjax: true
---
## 了解SVM (Support Vector Machine--支持向量机)
### 线性可分数据以及线性不可分数据
SVM(Support Vector Machine)是支持向量机，是常见的一种判别方法。在机器学习领域，是一个有监督的学习模型，通常用来进行模式识别、分类以及回归分析。
### 对于线性可分数据，从KNN到SVM
通过上一章的KNN，我们已经了解到，它可以对数据进行分类处理了：思想就是通过计算每个元素与待分类元素的距离来进行判断。这样做的缺点是需要大量的计算和存储（因为需要计算每个点到待测点的距离）;而SVM就是为了解决这样的问题而诞生的.如下图:
![svm_basics](./svm_basics1.png)
考虑这种情况：我们有一条线，它的函数形式为：$f(x) = ax_{1}+bx_{2}+c$（x为测试数据）, 通过这条线，可以将数据划分成两个部分。（如上图中的线），当我们新得到一个测试数据$X$的时候，只需要将$X$代入到函数$f(x)$中进行计算。如果$f(x) > 0$ 这个测试样本就属于蓝色的类别，否则它属于红色的类别。我们把这个函数对应的直线叫做：**决策边界(Decision Boundary)**，这种判断的形式非常简单，而且节省内存。
**像这类我们可以通过使用一条直线（或者高维的超平面）将其分为两部分的数据，我们称其为：线性可分数据。**
所以，在上面的图片中我们可以找到很多这样的直线，那么，采用哪一条直线是合理的呢？答案是：距离两侧的点越远越好，这样是最好的分类边界(目的是对抗噪声)；因此，SVM所做的就是找到一条直线（或超平面），最大化它与训练样本的距离。类似图像中通过中心的粗线。
![svm_basics2](./svm_basics2.png)
**为了找到这个角色边界，我们需要训练数据，但是我们需要全部训练数据吗？不是的，事实上，我们仅需要两个类别相邻处的一部分数据就可以了。**在我们的上图中的例子中，两个类别边界的的临近数据我们选择的分别是两个红色的方块和一个蓝色的圆圈。我们把他们叫做：支持向量（Support Vectors），通过他们的线（虚线）叫做：支持平面。(Support Planes)，通过他们足以发现我们需要找到的决策边界。
举个例子：蓝色数据是通过$w^{T}x+b_{0}>1$来表示的，同时红色数据是由$w^{T}x+b_{0}<-1$来表示的。（$w$是权重向量$(w=[w_{1},w=_{2},w_{3},...,w_{n}])$）同时$x$是特征向量$(x=[x_{1},x_{2},x_{3},...,x_{n}])$。$b_{0}$ 是 $bias$偏置项。权重向量决定决策边界的方向，偏置项决定决策边界的位置。决策边界的表示为：$w^{T}x+b_{0}=0$ 从支持向量到决策边界的最小距离由下式给出$distance_{support vectors}=\frac{1}{||w||}$``margin``是这个边界距离两倍，我们需要的是最大化的``margin``如图中的例子，我们需要找到了一个最小化的function$L(w,b_{0})$它的公式构成如下：
$$\min_{w,b_{0}}L(w,b_{0} = \frac{1}{2}||w||^{2})subject\ to\ t_{i}(w^{T}x+b_{0})\geqslant1\forall{i}$$
其中$t_{i}$是每一个类的标签label，$t_{i}\in [-1,1]$

### 对于线性不可分数据