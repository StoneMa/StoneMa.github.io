---
title: 从ANN到CNN
date: 2018-05-29 13:09:53
tags: [CNN,卷积神经网络]
categories: [人工智能,机器学习]
mathjax: true
---
**写在前面：** 或许机器学习这部分内容不应该就这么展开，应该循序渐进，待自己有了深层次了解以后，从基本的回归、分类讲起，但是最后还是选择了边学习边总结的形式，至少能详细记录自己曾经踩过的坑，记录自己的学习路线，也算是个好的形式吧！

---
周末跟北京的兄弟们聚了一波，每次见他们的感觉都特别的不一样，每次见完面后都感觉自己受了很大鼓舞，有无限动力。W哥好像一直都是这么的有激情有干劲儿，YL好像一直这么的能钻研从来不觉得累，而且他们的努力程度远在我认识的其他人之上！一起加油~

## **正文：**
## 引言
我相信很多人跟我一样，接触深度学习的最初原因是它很火听起来很厉害，而对于具体是如何实现的，自己根本闹不清……看一些论文、博客的时候都是看到一些类似下图的这样的图片，然后告诉自己：嗯~我大概了解深度学习的概念了。今天我们正儿八经的把它的皮扒开，认真分析它的经脉连通，深入了解什么是神经网络！什么是卷积神经网络，又是如何延伸到深度学习的！
![cnn](./cnn0.png)

这次我们从ANN（人工神经网络）讲解到 CNN（卷积神经网络）带你认识最基本的神经元，然后了解CNN卷积神经网络是如何工作的，然后展开讲它的每一层功能！为了保证我的思路不混乱，保证讲解的路线的正确性，我先以自己踩过的坑为带入点，然后依次深入。
## 1 从入坑开始 —— CNN
或许这并不是你接触到的深度学习领域，或者是机器学习领域的第一个专业名词，但是搞明白它，足以让我们学习到很多知识以供后续的学习和了解。
CNN（Convolutional Neural Networks）也就是我们常说的卷积神经网络，单单从这个名词来看，这里就有几个概念需要我们来弄明白——什么是**卷积**，什么是**神经网络**。
### 1.1 卷积
最早接触这个词的时候是读大学的时候学习《数字信号处理》这门课程，里面在FFT快速傅里叶变化的时候有讲到卷积运算，$\bigotimes$ 表示卷积运算，那么：
$$
f(x) \bigotimes g(x) = \int_{-\infty}^{+\infty} f(\tau)g(x-\tau)d\tau\tag{$1$} 
$$
离散化的卷积运算：
$$
y(n) = \sum_{i=-\infty}^{+\infty}x(i)h(n-i)=x(n)*h(n)\tag{$2$} 
$$
但是！！！这里神经网络中的卷积运算，并非傅里叶变换中的卷积运算，这里的卷积运算是以图像和卷积核（也叫滤波器）对应位置相乘再相加得到的结果：如下图所示：
![cnn](./CNN.gif)
图中的黄色区域的标注数字为卷积核，如下所示：
$$
 \left[
 \begin{matrix}
   1 & 0 & 1 \\
   0 & 1 & 0 \\
   1 & 0 & 1
  \end{matrix}
  \right]\tag{$3$} 
$$
我们可以看到，右边得到的结果是左边大矩阵和小矩阵卷积得到的，大矩阵就是我们的原图像，里面的值就是图像像素的值，而小矩阵就是卷积核。两者对应位置相乘然后依次相加，得到新矩阵的一个位置的值，然后向右滑动卷积核（滑动的间隔可调）得到新的位置的值。这就是神经网络中的**卷积**。这里要注意的是，我们从上图中可以看到，卷积核在移动的过程中是一个一个像素滑动的，这个滑动的间隔叫做**步长**（strides），strides的值可以自己设置，表示滑动的间隔的大小。通过这样的操作，我们可以直观的看到，原图像尺寸被卷积操作**缩小**了。这就是卷积操作的作用，可以通过卷积核把一个小区间内的**特征**提取出来，并以数值化的形式表现出来。如果你要深挖为什么要用这样的计算形式来处理卷积核与图形，建议看图像滤波方面的内容。
### 1.2 神经网络
好~我们初步解决了一个小问题，那么已经开始上道了，继续分析表象——**神经网络**，这个词~听起来真的高大上有木有！！第一次听到的时候也是上大学的时候，那时候先进一点的同学都在写神经网络算法，学什么蚁群算法，支持向量机，balabala~~一大堆在当时根本听不懂的名词~那，人总是要进步的，当时不愿意学，研究生再不学就有点耍流氓了~这里我们先介绍什么是**神经网络**。
### 1.2.1 神经元
如果你和我一样，是想从0开始，把所有概念全弄懂搭建自己的知识脉络，而不仅仅是关心应用层的东西，我希望你可以看看这部分，看看我这个愚钝的人是如何坑自己的，如果你早已对这些基本概念了解的熟烂于心，建议跳过。
**神经元**，这个词乍听起来怎么都不能和计算机专业联系在一起，它怎么也得是个生物学相关的东西啊！没错，神经元本就是生物学的概念，第一个人造神经元是1943年由Warren McCulloch和Walter Pitts首次提出的，最早用来作为大脑中“神经网络”的计算模型。如果你不清楚神经元的工作原理，请看下面这个图，神经元有很多个输入端，当输入的信号经过神经元的细胞核处理超过某个阈值后，就把生成的信号向下传递。
![神经元](./Neuron.png)
那么知道了真正的神经元的样子，人工神经元实际上就是模拟的生物学中的神经元，如下图所示，左侧的就是输入，中间圆圈就是处理单元，后面的就是输出：
![人工神经元](./SingleNeuron.png)
看到这个图，总的来讲我们要有一个概念，这个东西基本表示的是：有多个输入单元，然后将这些输入单元经过某种运算以后得到一个输出。
上图这个神经元的输入值是$x_{1},x_{2},x_{3}$以及截距b这里是 $+1$,中间是运算单元，输出是$h_{W,b}(x) = f(W^{T}x)=f(\sum^{3}_{i=1}W_{i}x_{i}+b)$,其中函数$f:\Re\longmapsto\Re$被称**激活函数**。
到这里，其实对于一个非数学或者计算机专业的同学来说已经有点懵了……（其实即使是数学和计算机专业的我，一开始也是懵的……）那么这里这个函数有什么作用呢？我们用一个例子来说明：比如我们有个数据a它的形式是这样的 {$x^{0},1$}，现在我们想通过一个函数能够使得$f(x^{0}) = 1$那么这个函数$f(x)$就是我们要研究的对象，也就是我们的模型！只不过这里是最简单的一维数据模型，那么让我们把原始输入复杂一点，假设输入是多维数据，（可以用向量来表示）那么就成了$(\boldsymbol x^{(0)},1)$ 也就是说对于这个输入进来的向量，我需要给每个维度的数据配比不同的权重来使得$f(\boldsymbol x^{(0)}) = 1,$;到这里依然很好理解，但是这里还是在讲单一数据，那么对于数据集呢？假设我们有这样的数据集$((\boldsymbol x^{i}),\boldsymbol y^{(i)})$,那我们就无法使用一个神经元来解决问题了，一个不行的话，就多个！那么多个神经元也就组成了我们的神经网络。
### 1.2.2 感知器（Perceptron）
其实人工神经元就是感知器，那么为什么又要引入这个名词呢？说起来尴尬，我起初认为这两个并不是一个东西，还仔细的区分两者，但是随着接触的东西多了，发现这帮定义规则的美国佬并没有想象中的那么严谨……其实感知器就是神经元，只不过在AI寒冬（发现感知器不能处理异或问题）之前他们管它叫感知器，而最近这些年人工智能火起来了，为了和之前的感知器区分，就起了神经元的名字。
### 1.2.3 人工神经网络(ANN)
![ANN](./ANN.png)
## 2 顺利入坑
如果看到了这里，恭喜你，已经入坑了，已经有了最基本的概念了，那么让我们继续修行。
### 2.1 什么是异或问题？
### 2.2 卷积神经网络中的分层
### 2.3 输入层（Input Layer）
### 2.4 卷积层（Convention Layer）
### 2.5 池化层（Sampling Layer，Pooling Layer）
### 2.6 全连接（Full-Connection Layer）
### 2.7 输出层（Output Layer）
## 3 KO，下一位
不得不说，就笔者这种烂笔法，如果你还能看到这里，我敬你是条汉子~ 让我们继续探索吧！毕竟到这，也基本才是刚刚了解了一点皮毛，还没有挖到神经网络的精髓。要继续的话，我们有必要了解一下前馈神经网络和反馈神经网络了，后者也就是我们常听到的BP神经网络。
### 3.1 前馈神经网络
### 3.2 BP神经网络
### 3.2.1 展开讲反向传递
## 4 其他网络
## 5 深度学习
## 6 参考
## 7 代码实现