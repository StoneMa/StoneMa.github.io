---
title: "熵","信息熵","交叉熵"
date: 2018-03-27 16:55:03
tags: [机器学习,基础知识]
categories: 机器学习
mathjax: true
---
**前言：**为什么要写这一篇文章呢？大概是因为自己的脑子太笨了吧……以前就问过龙哥这个东西怎么理解，他给了我一套公式: $H(p,q) = -\sum_xp(x) \log q(x)$ 瞅了半天，发现以自己的智商并不能从公式中总结出什么门道儿，所以有感而发："熵"这个字是特么谁发明的……内心的焦灼已经按耐不住了，既然要学，就学个明明白白，刨根问底！文章将陆续更新直至更新不动……
### 什么是熵？(Entropy)
熵的英文单词为：**Entropy**。'熵'是热力学中表征物质状态的参量之一，用符号S表示，其物理意义是体系混乱程度的度量。我们在中学时期学习热力学第二定律，
- **热力学第二定律（second law of thermodynamics）：**不可能把热从低温物体传到高温物体而不产生其他影响，或不可能从单一热源取热使之完全转换为有用的功而不产生其他影响。
其实我们学到的是删减版的内容，真实的热力学第二定律还有一条：
- **不可逆热力过程中熵的微增量总是大于零。又称“熵增定律”，表明了在自然过程中，一个孤立系统的总混乱度（即“熵”）不会减小。**
**熵越大，表示系统月无序，熵越小，表示系统越有序。**
如何理解这句话呢？不妨在仔细揣摩一下前面的第二定律，我们知道热能是不可能完全用来做功的，因为有热散失，这部分散失就表示我们的系统是不可逆的，这就是熵增的过程。也就是说：如果一个系统没有外接干扰，肯定是向整体能量变低的方向运动的，也就是熵值变大。对任何已知孤立的物理系统的演化，热熵只能增加，不能减少。

### 什么是信息熵？(Information Entropy)
信息熵是香农引入到信息论当中的计量单位。用来衡量一个随机变量出现的希望值。香农提出了用信息熵来定量衡量信息的大小，就像热力学中一样，概率越大的事件，信息熵越小；概率越小的事件，信息熵越大。在信息世界，熵越高（大），则能传输越多的信息，熵越低（小），则意味着传输的信息越少。然而这里与热熵相反的是，信息熵只能减少，不能增加。
举一个简单的例子来体会在生活中什么是信息熵：
比如：你在炒股，正在10支股票中犹豫不决，而这时候我作为某公司内部人员告诉你：第3支股票明天涨停。这时候我说的这句话就有很高的信息熵；再比如：我告诉你太阳明天从东方升起。这句话的信息熵就很低，因为它几乎是概率为1的确定性事件。
所以到这里，我们明白了：**所谓信息熵高，就是说，我得到某个信息后，事件的不确定性大幅度降低了。** 反观“太阳从东边升起” 这句话，从信息论的角度讲，并没有消除任何不确定性，所以它的信息熵几乎为0。
信息熵不仅定量衡量了信息的大小，同时也为信息编码提供了理论上的最优值：编码平均长度的理论下届就是**信息熵**。即：信息熵为数据压缩的极限。

#### 如何计算信息熵？
我们以一个例子来说明。

比如说：百米赛跑，有四位选手参加,分别是：$ \{A, B, C, D  \}$,他们获胜的概率分别为：$ \{ \frac{1} {2}, \frac{1} {4}, \frac{1} {8}, \frac{1} {8} \}$。接下来，我们将**“谁能获胜”**视为一个随机变量 $X\in\{A,B,C,D\}$ 。假定我们需要用尽可能少的二元问题来确定随机变量 X 的取值。

例如：

问题1：A获胜了吗？

问题2：B获胜了吗？问题3：

C获胜了吗？

最后我们可以通过最多3个二元问题，来确定 $X$ 的取值，即谁赢了比赛。

如果 $X=A$ ，那么需要问1次（问题1：是不是$A$？），概率为$ \frac{1}{2} $；

如果$ X=B$ ，那么需要问2次（问题1：是不是$A$？问题2：是不是$B$？），概率为$ \frac{1}{4} $；

如果 $X=C$，那么需要问3次（问题1，问题2，问题3），概率为$ \frac{1}{8} $;

如果 $X=D$ ，那么同样需要问3次（问题1，问题2，问题3），概率为$ \frac{1}{8} $；

那么很容易计算，在这种问法下，为确定 $ X $ 取值的二元问题数量为：

$$ E(N) = \frac {1} {2}\cdot 1+\frac{1} {4}\cdot2 + \frac{1} {8} \cdot 3 + {1} {8} \cdot 3 =\frac {7} {4} $$

那么我们回到信息熵的定义，会发现通过之前的信息熵公式，神奇地得到了：

$$ H(X) = \frac {1} {2}\log(2) + \frac{1} {4} \log(4) + \frac{1} {8} \log(8)+ \frac{1} {8} \log(8) = \frac{7} {4} bits$$
	
### 在计算机领域中的应用
另一个稍微复杂的例子是假设一个随机变量$X$，取三种可能值$x_1,x_2,x_3$，概率分别为$\frac{1} {2}$，$\frac{1} {4}$，$\frac{1} {4}$，那么编码平均比特长度为：$\frac{1} {2}\times1+\frac{1} {4}\times2+\frac{1} {4}\times2 = \frac{3} {2} $ 因此信息熵为$\frac{3} {2}$

在二进制计算机中，一个比特为0或1，其实就代表了一个二元问题的回答。也就是说，在计算机中，我们给**“谁能夺冠”** 这个事件进行编码，所需要的平均码长为1.75个比特。

平均码长的定义为： 
$$ L(C)=\sum\limits_{x\in\mathcal{X}}p(x)l(x)$$
很显然，为了尽可能减少码长，我们要给发生概率 p(x) 较大的事件，分配较短的码长 l(x) 。这个问题深入讨论，可以得出**霍夫曼编码**的概念。

**因此信息熵实际是对随机变量的比特量和顺次发生概率相乘再总和的数学期望。**
### 跟机器学习又什么关系？

### 如何理解交叉熵？

$H(p,q) = -\sum_{x}p(x)\log q(x)$

[香农信息论](https://www.zhihu.com/question/27068465).
[什么是信息熵](https://www.zhihu.com/question/22178202).
