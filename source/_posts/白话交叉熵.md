---
title: 入门熵到了解交叉熵
date: 2018-03-27 16:55:03
tags: [机器学习,基础知识]
categories: 机器学习
mathjax: true
---
**前言：**为什么要写这一篇文章呢？大概是因为自己的脑子太笨了吧……以前就问过龙哥这个东西怎么理解，他给了我一套公式: $H(p,q) = -\sum_xp(x) \log q(x)$ 瞅了半天，发现以自己的智商并不能从公式中总结出什么门道儿，所以有感而发："熵"这个字是特么谁发明的……内心的焦灼已经按耐不住了，既然要学，就学个明明白白，刨根问底！文章将陆续更新直至更新不动……
### 什么是熵？(Entropy)
熵的英文单词为：**Entropy**。'熵'是热力学中表征物质状态的参量之一，用符号S表示，其物理意义是体系混乱程度的度量。我们在中学时期学习热力学第二定律，
- **热力学第二定律（second law of thermodynamics）：**不可能把热从低温物体传到高温物体而不产生其他影响，或不可能从单一热源取热使之完全转换为有用的功而不产生其他影响。
其实我们学到的是删减版的内容，真实的热力学第二定律还有一条：
- **不可逆热力过程中熵的微增量总是大于零。又称“熵增定律”，表明了在自然过程中，一个孤立系统的总混乱度（即“熵”）不会减小。**
**熵越大，表示系统月无序，熵越小，表示系统越有序。**
如何理解这句话呢？不妨在仔细揣摩一下前面的第二定律，我们知道热能是不可能完全用来做功的，因为有热散失，这部分散失就表示我们的系统是不可逆的，这就是熵增的过程。也就是说：如果一个系统没有外接干扰，肯定是向整体能量变低的方向运动的，也就是熵值变大。对任何已知孤立的物理系统的演化，热熵只能增加，不能减少。
### 什么是信息熵？(Information Entropy)
信息熵是香农引入到信息论当中的计量单位。用来衡量一个随机变量出现的希望值。香农提出了用信息熵来定量衡量信息的大小，就像热力学中一样，概率越大的事件，信息熵越小；概率越小的事件，信息熵越大。在信息世界，熵越高（大），则能传输越多的信息，熵越低（小），则意味着传输的信息越少。然而这里与热熵相反的是，信息熵只能减少，不能增加。
举一个简单的例子来体会在生活中什么是信息熵：
比如：你在炒股，正在10支股票中犹豫不决，而这时候我作为某公司内部人员告诉你：第3支股票明天涨停。这时候我说的这句话就有很高的信息熵；再比如：我告诉你太阳明天从东方升起。这句话的信息熵就很低，因为它几乎是概率为1的确定性事件。
所以到这里，我们明白了：**所谓信息熵高，就是说，我得到某个信息后，事件的不确定性大幅度降低了。** 反观“太阳从东边升起” 这句话，从信息论的角度讲，并没有消除任何不确定性，所以它的信息熵几乎为0。
信息熵不仅定量衡量了信息的大小，同时也为信息编码提供了理论上的最优值：编码平均长度的理论下届就是**信息熵**。即：信息熵为数据压缩的极限。
#### 如何计算信息熵？
如果有一枚理想的硬币，其出现正面和反面的机会相等，则抛硬币事件的熵等于其能够达到的最大值。我们无法知道下一个硬币抛掷的结果是什么，因此每一次抛硬币都是不可预测的。因此，使用一枚正常硬币进行若干次抛掷，这个事件的熵是一比特，因为结果不外乎两个——正面或者反面，可以表示为0, 1编码，而且两个结果彼此之间相互独立。若进行n次独立实验，则熵为n，因为可以用长度为n的比特流表示。但是如果一枚硬币的两面完全相同，那个这个系列抛硬币事件的熵等于零，因为结果能被准确预测。现实世界里，我们收集到的数据的熵介于上面两种情况之间。
### 在计算机领域中的应用
另一个稍微复杂的例子是假设一个随机变量$X$，取三种可能值$x_{1},x_{2},x_{3}$，概率分别为$\frac{1}{2}$，$\frac{1}{4}$，$\frac{1}{4}$，那么编码平均比特长度为：$\frac{1}{2}\times1+\frac{1}{4}\times2+\frac{1}{4}\times2 = \frac{3}{2} $ 因此信息熵为$\frac{3}{2}$
**因此熵实际是对随机变量的比特量和顺次发生概率相乘再总和的数学期望。**
### 跟机器学习又什么关系？

### 如何理解交叉熵？

$H(p,q) = -\sum_{x}p(x)\log q(x)$

[香农信息论](https://www.zhihu.com/question/27068465).
[什么是信息熵](https://www.zhihu.com/question/22178202).
