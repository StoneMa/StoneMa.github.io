---
title: 认识卷积神经网络CNN
date: 2018-06-04 14:53:29
tags: [卷积神经网络,机器学习,感知器]
categories: 神经网络
mathjax: true
---
#卷积神经网络
首先来看上一篇文章中的图，这个图就是用来描述CNN
![cnn](./cnn0.png)
## 从入坑开始 —— CNN
或许这并不是你接触到的第一个深度学习领域、或者是机器学习领域的专业名词，但是搞明白它，足以让我们学习到很多知识以供后续的学习和进步。
CNN（Convolutional Neural Networks）也就是我们常说的卷积神经网络，单单从这个名词来看，这里就有几个概念需要我们来弄明白——什么是**卷积**，什么是**神经网络**。

## 卷积
最早接触这个词的时候是读大学的时候学习《数字信号处理》这门课程，里面在FFT快速傅里叶变化的时候有讲到卷积运算，$\bigotimes$ 表示卷积运算，那么：
$$
f(x) \bigotimes g(x) = \int_{-\infty}^{+\infty} f(\tau)g(x-\tau)d\tau\tag{$1$} 
$$
离散化的卷积运算：
$$
y(n) = \sum_{i=-\infty}^{+\infty}x(i)h(n-i)=x(n)*h(n)\tag{$2$} 
$$
但是！！！这里神经网络中的卷积运算，并非傅里叶变换中的卷积运算，这里的卷积运算是以图像和卷积核（也叫滤波器）对应位置相乘再相加得到的结果：如下图所示：
![cnn](./CNN.gif)
图中的黄色区域的标注数字为卷积核，如下所示：
$$
 \left[
 \begin{matrix}
   1 & 0 & 1 \\
   0 & 1 & 0 \\
   1 & 0 & 1
  \end{matrix}
  \right]\tag{$3$} 
$$
我们可以看到，右边得到的结果是左边大矩阵和小矩阵卷积得到的，大矩阵就是我们的原图像，里面的值就是图像像素的值，而小矩阵就是卷积核。两者对应位置相乘然后依次相加，得到新矩阵的一个位置的值，然后向右滑动卷积核（滑动的间隔可调）得到新的位置的值。这就是神经网络中的**卷积**。这里要注意的是，我们从上图中可以看到，卷积核在移动的过程中是一个一个像素滑动的，这个滑动的间隔叫做**步长**（strides），strides的值可以自己设置，表示滑动的间隔的大小。通过这样的操作，我们可以直观的看到，原图像尺寸被卷积操作**缩小**了。这就是卷积操作的作用，可以通过卷积核把一个小区间内的**特征**提取出来，并以数值化的形式表现出来。如果你要深挖为什么要用这样的计算形式来处理卷积核与图形，建议看图像滤波方面的内容。

## 卷积神经网络中的分层
## 输入层（Input Layer）
## 卷积层（Convention Layer）
## 池化层（Sampling Layer，Pooling Layer）
## 全连接（Full-Connection Layer）
## 输出层（Output Layer）

---
有了上面对卷积神经网络的了解，我们要继续了解一下其他内容，下面的前馈网络和BP网络都是对网络中权值更新的算法。
## 前馈神经网络
## BP神经网络
首先不要认为BP网络与卷积网络是并列关系，两者专注的领域不同，BP网络的目的是使用反向传递的思想来更新权值，而卷积网络的目的是降低网络中神经元的连接数量从而减少权值的计算。
### 展开讲反向传递
# 其他网络
# 深度学习
# 参考
[卷积神经网络](./https://www.jianshu.com/p/fe428f0b32c1)